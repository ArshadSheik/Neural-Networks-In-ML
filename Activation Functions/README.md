# Activation Functions in Neural Networks  

Activation Functions are functions used in neural networks for calculating the weighted sum of input and biases. Itâ€™s done to determine if a neuron should be activated or not. What happens is that the input layer receives the training data for training the neural network, the hidden layer is used to do all the calculations which help in finding relationships between the features of the data, and then the output layer gives the results which are controlled by the activation function.  

The position of an activation function in neural network architectures depends on its functions. They are placed after the hidden layers to produce the results for the neural network. Different types of activation functions are used in neural network architectures.  

&nbsp;

# Types of Activation Functions


## **Sigmoid Function**

The sigmoid activation function is a nonlinear activation function commonly used in feedforward neural networks. They are used to train neural network architectures to predict probability. Thus, **a sigmoid activation function can be used for binary classification problems.**  


## **Hyperbolic Tangent Function**  

Another type of activation function used in neural networks is the hyperbolic tangent function. It is also known as the `tanh` function. **It is commonly used in recurrent neural network architectures to solve problems based on natural language processing.**  


## **Softmax Function**

The softmax function is a popular activation function used in neural network architectures. Like the sigmoid activation function, the softmax function is also used in training models to predict probability. The difference is that the sigmoid function is used in binary classification problems, and **the softmax function is used in multiclass classification problems.**  


## **Rectified Linear Unit Function (ReLU)**

The rectified linear unit or `ReLU` is the most widely used activation function in neural network architectures. It is a faster activation function and has better performance and generalization compared to all other activation functions. **The ReLU activation function is widely used in deep neural network architectures to solve problems such as object recognition and speech recognition.**  


More About [Activation Functions](https://arxiv.org/pdf/1811.03378.pdf)
